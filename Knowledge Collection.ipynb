{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Collection\n",
    "\n",
    "### Kernels \n",
    "* Here “kernel” is being used in the sense of positive semi-definiteness or that k(u,u) ≥ 0 for all u\n",
    "* Kernels are the idea of summing functions that imitate similarity (induce a positive-definite encoding of nearness).\n",
    "* Kernels are a combination of two good ideas, they have one important property and are subject to one major limitation.\n",
    "* Optimization problems that use the kernel as their encoding are well formed. You can “regularize” optimization problems with a kernel penalty because it behaves a lot like a norm. Without the positive semidefinite property all of these optimization problems would be able to “run to negative infinity” or use negative terms (which are not possible from a kernel) to hide high error rates. \n",
    "* The limits of the kernel functions (not being able to turn distance penalties into bonuses) help ensure that the result of optimization is actually useful (and not just a flaw in our problem encoding).\n",
    "*\n",
    "\n",
    "### Kernel Trick\n",
    "* if it is usable only one place it is a trick, if it is usable multiple places it is a technique.\n",
    "* Adding new features/variables that are functions of your other input variables can change linearly inseparable problems into linearly separable problems. For example if our points were encoded not as $ u(i) = (x(i),y(i)) $ but as <br /> <br /> $$ u(i) = (x(i),y(i),x(i)*x(i),y(i)*y(i),x(i)*y(i)) $$ <br /> we could easily find the exact concept ( y(i) > x(i)*x(i) which is now a linear concept encoded as the vector (0,1,-1,0,0).\n",
    "* Often you don’t need the coordinates of u(i). You are only interested in functions of distances $||u(i)-u(j)||^2$ and in many cases you can get at these by inner products and relations like <br /> <br />\n",
    "$$ ||u(i)-u(j)||^2 = <u(i),u(i)> + <u(j),u(j)> – 2<u(i),u(j)> $$.\n",
    "* The important property is that kernels look like inner products in a transformed space. The definition of a kernel is: there exists a magic function phi() such that for all u,v:<br /> <br />\n",
    "$$k(u,v) = <\\phi(u),\\phi(v)>$$ <br />This means that k(.,.) is behaving like an inner product in some (possibly unknown) space. The important consequence is the positive semi-definiteness, which implies k(u,u)≥0 for all u (and this just follows from the fact about inner products over the real numbers that <z,z>≥0 for all z).\n",
    " \n",
    "\n",
    "### Support Vector Machines\n",
    "* By “best” we will mean “best margin” (to be defined later) because with “best margin” as our objective the optimization problem of solving for the best data weights has a particularly beautiful form that can be reliably solved at great scale. This is called a “support vector model” (or support vector machine)\n",
    "* Nearest neighbor classifiers are optimal in the sense that with an infinite amount of data the 1-nearest neighbor classifier has an error rate that approaches twice the Bayes error rate (the Bayes error rate being the ideal error observed on identical repetitions, or the theoretical best error rate) and for large k the k-nearest neighbor method approaches the Bayes error rate itself\n",
    "* The support vectors are the datums with non negligible (highly important) weights\n",
    "* A consequence is: for support vector machines if b is non-zero (as it almost surely will be) and the kernels all go to zero as we approach infinity fast enough (as they are designed to do) then exactly one of the learned classes is infinite and the other is a union of islands (regardless if this was true for the training data). [The colors red and blue are picked if the sum is above or below a constant “b” called “the dc-term” (part of the support vector solution).  It is a bias also shared by support vector machines with Gaussian kernels].\n",
    "\n",
    "### Margin\n",
    "* Margin is in fact a posterior observation. That is: margin is observed after the training data is seen, not known before data is seen (like, for example, Vapnik-Chervonenkis dimension). Margin is useful as it bounds generalization error but it is not the prior bound it is often portrayed as. So we assert margin estimates are not much more special than simple cross-validation estimates which can also be performed once we have data available.\n",
    "\n",
    "### Generalization Error\n",
    "* Generalization error is an effect of “over fitting” where a model has learned things that are true about the training examples that do not hold for the overall truth or concept we are trying to learn (i.e. don’t generalize). Generalization error is the excess error rate we observe when scoring new examples versus the error-rate we saw in learning the training data.\n",
    "\n",
    "### Radial Basis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
